base:
  project_name: salamandra-cot
  log_level: DEBUG
  models_dir: ../models
  datasets_dir: ../datasets
  checkpoints_dir: ./checkpoints
  generations_dir: ./generations
  evaluations_dir: ./evaluations

dataset_translate:
  translate_dir_path: translated
  hf_model_id: facebook/seamless-m4t-v2-large
  datasets:
    magpie_v2_llama3:
      dir_path: magpie_v2_llama3/data
      format: parquet
      hf_dataset_id: Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Llama3
      lang: en
      target_lang: es
      col_names: 
        instruction: instruction
        response: response

format_datasets:
  raw_dir_path: raw
  format_dir_path: formatted

fine_tune:
  training_dir_path: tuned
  hf_model_id: BSC-LT/salamandra-7b
  training_args:
    num_train_epochs: 10
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    save_steps: 500
    save_total_limit: 3
    learning_rate: 3e-4
    logging_steps: 1000
    warmup_steps: 500
    weight_decay: 0.01
    fp16: true
    gradient_accumulation_steps: 4
    report_to: none
    optim: adamw_torch

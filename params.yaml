base:
  project_name: salamandra-cot
  log_level: DEBUG
  models_dir: ../models
  datasets_dir: ../datasets
  checkpoints_dir: ./checkpoints
  generations_dir: ./generations
  evaluations_dir: ./evaluations

data:
  train:
    magpie_v2_llama3:
      dir_path: magpie_v2_skywork_o1/data
      format: parquet
      hf_dataset_id: Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B
      src_lang: English
      tgt_lang: Spanish
      col_names: # map column names to dataset schema
        instruction: instruction
        response: response

format_datasets:
  raw_dir_path: raw
  format_dir_path: formatted

dataset_translate:
  translate_dir_path: translated
  checkpoint_step: 500
  model:
    max_length: 1024
    dir_path: salamandrata_2b
    hf_model_id: BSC-LT/salamandraTA-2B
    prompt_template: "[{src_lang}] {stmt} \n[{tgt_lang}]:"
    model_args:
      num_return_sequences: 1
      do_sample: true
      num_beams: 5
      top_p: 0.95
      repetition_penalty: 1.2
      no_repeat_ngram_size: 3
      early_stopping: true
      use_cache: true

fine_tune:
  training_dir_path: tuned
  hf_model_id: BSC-LT/salamandra-7b
  training_args:
    num_train_epochs: 10
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    save_steps: 500
    save_total_limit: 3
    learning_rate: 3e-4
    logging_steps: 1000
    warmup_steps: 500
    weight_decay: 0.01
    fp16: true
    gradient_accumulation_steps: 4
    report_to: none
    optim: adamw_torch

base:
  project_name: salamandra-cot
  log_level: DEBUG
  models_dir: ../models
  datasets_dir: ../datasets
  checkpoints_dir: ./checkpoints
  generations_dir: ./generations
  evaluations_dir: ./evaluations

data:
  train:
    magpie_v2_llama3:
      dir_path: magpie_v2_llama3/data
      format: parquet
      hf_dataset_id: Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Llama3
      src_lang: eng
      tgt_lang: spa
      col_names: # map column names to dataset schema
        instruction: instruction
        response: response

format_datasets:
  raw_dir_path: raw
  format_dir_path: formatted

dataset_translate:
  translate_dir_path: translated
  checkpoint_step: 500
  model:
    dir_path: madlad400_3b_mt
    hf_model_id: google/madlad400-3b-mt

fine_tune:
  training_dir_path: tuned
  hf_model_id: BSC-LT/salamandra-7b
  training_args:
    num_train_epochs: 10
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    save_steps: 500
    save_total_limit: 3
    learning_rate: 3e-4
    logging_steps: 1000
    warmup_steps: 500
    weight_decay: 0.01
    fp16: true
    gradient_accumulation_steps: 4
    report_to: none
    optim: adamw_torch
